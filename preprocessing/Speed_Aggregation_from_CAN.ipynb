{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#CAN_PATH = '/data2/jiang/Toyota/CAN data/0701ver/'\n",
    "#NEW_CAN_PATH = '/data2/jiang/Toyota/CAN_day_data/'\n",
    "OCT_CAN_PATH = '/data2/jiang/Toyota/CAN_day_data_202110/'\n",
    "NOV_CAN_PATH = '/data2/jiang/Toyota/CAN_day_data_202111/'\n",
    "MAY_CAN_PATH = '/data2/jiang/Toyota/CAN_day_data_1122ver/'\n",
    "CAN_AGG_PATH = '/data2/jiang/workToyota/data/CAN_Aggregated/'\n",
    "#NEW_ACC_PATH = '/data2/jiang/Toyota/JARTIC_data_0721ver/'\n",
    "#NEW_LINK_PATH = '/data2/jiang/workToyota/data/'\n",
    "#INT_CAN_PATH = '/data2/jiang/workToyota/data/ACC_CAN_data/'\n",
    "#DAYS = [date.strftime('%Y-%m-%d') for date in pd.date_range(start='2021-05-10', end='2021-06-01', freq='1D')]\n",
    "SAMPLING_RATE = '10min'      #Defining time-interval for aggregation\n",
    "#SPEED_DROP_THRESHOLD = 0.75\n",
    "LAT_MIN = 35.36\n",
    "LAT_MAX = 35.90\n",
    "LON_MIN = 139.537\n",
    "LON_MAX = 139.947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract linkids in give rectangular coordinates\n",
    "def clip_CAN(mask):\n",
    "    #df = pd.read_csv(os.path.join(NEW_LINK_PATH, 'link_connect_all.csv'))\n",
    "    #df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['start_lon'], df['start_lat'])).set_crs(epsg=4326)\n",
    "    dft = df[(df['mmlatitude']>=LAT_MIN) & (df['mmlatitude']<=LAT_MAX) & (df['mmlongitude']>=LON_MIN) & \n",
    "            (df['mmlongitude']<=LON_MAX)]\n",
    "    #df = gpd.clip(df, mask)\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute the average speed profile for given linkid for given accident and show the plot for the same\n",
    "def plot_speed(df_acc_adverse, df_can_group, link_neighbor_list_adv):\n",
    "    \n",
    "    #Conversion into sampling rate points\n",
    "    accident_time_list = df_acc_adverse.start_time.values\n",
    "    accident_time_list = pd.to_datetime(accident_time_list)\n",
    "    \n",
    "    linkid_list = df_acc_adverse.coord_start_upstream_nearestlink.values\n",
    "    \n",
    "    #Plotting accident profiles row by row\n",
    "    for accident_time, linkid, linkid_neighbor, end_time, idx in zip(accident_time_list, linkid_list, link_neighbor_list_adv, \n",
    "                                                      df_acc_adverse.end_time.values, df_acc['Unnamed: 0'].values):\n",
    "        \n",
    "        \n",
    "        ##Plotting speed chart for accident linkid for +-2.5 days\n",
    "        start_time = accident_time\n",
    "        accident_time = accident_time.floor(SAMPLING_RATE)\n",
    "        \n",
    "        #Creating time-slices +-2.5 days before/after the accident\n",
    "        timeslices = pd.date_range(accident_time-dt.timedelta(days=2,hours=12), accident_time+dt.timedelta(days=2,hours=12), \n",
    "                               freq=SAMPLING_RATE)\n",
    "    \n",
    "        #Extracting relevant GPS Points\n",
    "        tmp=df_can_group[df_can_group.linkid==linkid].set_index('gps_timestamp').reindex(timeslices).reset_index()\n",
    "        tmp['speed_typea'] = tmp['speed_typea'].interpolate(limit=15,limit_direction='both')\n",
    "        #tmp.fillna(0)\n",
    "        \n",
    "        #Plotting the accidents profile\n",
    "        data = tmp['speed_typea'].values\n",
    "        accident_time_str = accident_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        accident_time_hour = accident_time_str[-8:]\n",
    "    \n",
    "        ticks = [date.strftime('%Y-%m-%d %H:%M:%S') for date in timeslices]\n",
    "        all_hour_int = [i for i, x in enumerate(ticks) if accident_time_hour in x]\n",
    "        all_hour_str = [x for x in ticks if accident_time_hour in x]\n",
    "    \n",
    "        fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(25,22), gridspec_kw={'height_ratios':[10,10]}) \n",
    "        ax0.plot(data)\n",
    "        \n",
    "        for i,hour in zip(all_hour_int, all_hour_str):\n",
    "            if hour == accident_time_str:\n",
    "                ax0.axvline(x=i, label=f'{hour}_accident', c='r')\n",
    "            else:\n",
    "                ax0.axvline(x=i, label=hour, c='g')\n",
    "        ax0.legend(loc=\"upper left\")\n",
    "        ax0.set_title(f\"link id {linkid} accident time{start_time.strftime('%Y-%m-%d %H:%M:%S')}\", fontsize=12, y=1)\n",
    "        ax0.set_xticks(np.arange(data.shape[0]))\n",
    "        ax0.set_xticklabels(ticks, rotation=90)\n",
    "        ax0.set_xlabel('Time')\n",
    "        ax0.set_ylabel('Value')\n",
    "        \n",
    "        \n",
    "        ##Plotting Heatmap like graph for the day of the accident\n",
    "        DAY = accident_time.strftime('%Y-%m-%d')\n",
    "        inds = pd.date_range(pd.Timestamp(DAY), pd.Timestamp(DAY)+pd.Timedelta('1D'),freq=SAMPLING_RATE,closed='left')\n",
    "        \n",
    "        #Getting data in speed-linkid matrix format \n",
    "        spd = df_can_group[df_can_group.linkid.isin(linkid_neighbor) & \n",
    "                                (df_can_group.gps_timestamp.dt.date==accident_time.date())]\n",
    "        spd = spd.pivot(index='gps_timestamp',columns='linkid',values='speed_typea')\n",
    "        spd = spd.reindex(columns=linkid_neighbor)\n",
    "        spd = spd.reindex(inds)\n",
    "        spd.interpolate(axis=1,limit=15,limit_direction='both',inplace=True)\n",
    "        \n",
    "        \n",
    "        inds = pd.date_range(pd.Timestamp(DAY), pd.Timestamp(DAY)+pd.Timedelta('1D'),freq=SAMPLING_RATE,closed='left')\n",
    "        cols = linkid_neighbor\n",
    "        sns.set_theme(style=\"white\")\n",
    "\n",
    "        st = accident_time.floor(SAMPLING_RATE)\n",
    "        st_ind = inds.indexer_between_time(st.time(),(st+pd.Timedelta('2min')).time())\n",
    "        et = pd.Timestamp(end_time).ceil(SAMPLING_RATE)\n",
    "        et_ind = inds.indexer_between_time(et.time(),(et+pd.Timedelta('2min')).time())\n",
    "        \n",
    "        ax1.set_yticks(np.arange(len(cols)))\n",
    "        ax1.set_yticklabels(cols)\n",
    "        ax1.set_xticks(np.arange(inds.shape[0]))\n",
    "        ax1.set_xticklabels(inds, rotation=90)\n",
    "        ax1.axhline(y=cols.index(linkid), label=f'accident', c='r')\n",
    "        ax1.axvline(x=st_ind, label=f'accident', c='r')\n",
    "        ax1.axvline(x=et_ind, label=f'accident', c='r')\n",
    "        \n",
    "        #im = ax1.imshow(spd.values.T, interpolation='bilinear',cmap='RdYlGn')\n",
    "        im = ax1.imshow(spd.values.T, cmap='RdYlGn')\n",
    "        \n",
    "        divider = make_axes_locatable(ax1)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "        plt.colorbar(im, cax=cax)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        fig.savefig(NEW_LINK_PATH+'Adverse_Accidents_Plots/'+str(idx)+': linkid='+str(linkid)+\n",
    "                            ',accident-time='+start_time.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-11cc93eb0fde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#Reading the file, dropping NA values, and clipping it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed_typea'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_CAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m     \"\"\"\n\u001b[1;32m   1462\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main1 (Aggregating the avg speed for each linkid and saving in monthly files (without reindexing))\n",
    "\n",
    "mask = Polygon([(LON_MIN,LAT_MAX), (LON_MAX,LAT_MAX), (LON_MAX,LAT_MIN), (LON_MIN,LAT_MIN)])\n",
    "month_list = ['MAY', 'OCT', 'NOV']\n",
    "i = 0\n",
    "\n",
    "#Reading the files\n",
    "for PATH_VAR in [MAY_CAN_PATH, OCT_CAN_PATH, NOV_CAN_PATH]:\n",
    "    files = [os.path.join(PATH_VAR+filename) for filename in os.listdir(PATH_VAR)]\n",
    "    files.sort()\n",
    "    \n",
    "    tik = time.time()\n",
    "    df_month = []\n",
    "    for filename in files:\n",
    "        #Reading the file, dropping NA values, and clipping it\n",
    "        df = pd.read_csv(filename, compression='gzip')\n",
    "        df = df.dropna(subset=['speed_typea'])\n",
    "        df = clip_CAN(mask)\n",
    "        \n",
    "        #Sampling and getting avg speed after aggregation\n",
    "        df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "        df['gps_timestamp'] = df['gps_timestamp'].dt.floor(SAMPLING_RATE)    \n",
    "        df = df.groupby(['linkid','gps_timestamp'], as_index=False)['speed_typea'].mean()[['linkid',\"gps_timestamp\",\n",
    "                                                                                         \"speed_typea\"]]\n",
    "\n",
    "        #Reindexing timestamps\n",
    "        #current_date = df['gps_timestamp'].iloc[0].date()\n",
    "        #timeslices = pd.date_range(current_date, current_date+dt.timedelta(hours=24), freq=SAMPLING_RATE)[:-1]\n",
    "        #mux = pd.MultiIndex.from_product([df.linkid.unique(), timeslices],names=['linkid', 'gps_timestamp'])\n",
    "        #df = df.set_index(['linkid', 'gps_timestamp']).reindex(mux).reset_index()\n",
    "        \n",
    "        #Appending the one day result\n",
    "        df_month.append(df)\n",
    "    \n",
    "    del df\n",
    "    df_month = pd.concat(df_month)\n",
    "    df_month.to_csv(CAN_AGG_PATH+month_list[i]+'_CAN.csv.gz', compression='gzip', index=False)\n",
    "    i += 1\n",
    "    \n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/MAY_CAN.csv.gz\n",
      "129.92215299606323\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/NOV_CAN.csv.gz\n",
      "153.91129755973816\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/OCT_CAN.csv.gz\n",
      "156.29844331741333\n"
     ]
    }
   ],
   "source": [
    "#Main2\n",
    "#Getting common list of all linkids\n",
    "\n",
    "files = [os.path.join(CAN_AGG_PATH+filename) for filename in os.listdir(CAN_AGG_PATH)]\n",
    "files.sort()\n",
    "files = files[2:-1:2]\n",
    "\n",
    "linkid_list = []\n",
    "\n",
    "for filename in files:\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    df = pd.read_csv(filename, compression='gzip')\n",
    "    linkid_list = linkid_list + df.linkid.unique().tolist()\n",
    "    tok = time.time()\n",
    "    print(tok-tik)\n",
    "#del df\n",
    "linkid_list = np.unique(np.array(linkid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/OCT_CAN.csv.gz\n",
      "7237824\n",
      "2841\n",
      "12682224\n",
      "5444400\n",
      "364.59753036499023\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/MAY_CAN.csv.gz\n",
      "6478303\n",
      "2841\n",
      "12682224\n",
      "6203921\n",
      "330.33218908309937\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/NOV_CAN.csv.gz\n",
      "7224913\n",
      "2841\n",
      "12273120\n",
      "5048207\n",
      "349.8731253147125\n"
     ]
    }
   ],
   "source": [
    "#Main3\n",
    "#Extracting capital linkids only and doing the reindexing (monthly files)\n",
    "\n",
    "df_capital_link = pd.read_csv('/data2/jiang/Toyota/graph_data/capital_graph_link_info.csv')\n",
    "capital_linkid_list = df_capital_link['link_id'].unique()\n",
    "\n",
    "files = [os.path.join(CAN_AGG_PATH+filename) for filename in os.listdir(CAN_AGG_PATH)]\n",
    "for filename in files:\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    df = pd.read_csv(filename, compression='gzip')\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[df['linkid'].isin(capital_linkid_list)]\n",
    "    print(len(df))\n",
    "    \n",
    "    df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "    first_date = df['gps_timestamp'].iloc[0].date()\n",
    "    last_date = df['gps_timestamp'].iloc[-1].date()\n",
    "    timeslices = pd.date_range(first_date, last_date+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "    \n",
    "    mux = pd.MultiIndex.from_product([capital_linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "    df = df.set_index(['linkid', 'gps_timestamp']).reindex(mux).reset_index()\n",
    "    df.to_csv(filename[:-7]+'_CAPITAL.csv.gz', compression='gzip', index=False)\n",
    "    \n",
    "    print(df.linkid.nunique())\n",
    "    print(len(df))\n",
    "    print(df.speed_typea.isna().sum())\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/MAY_CAN.csv.gz\n",
      "22719.928786754608\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/NOV_CAN.csv.gz\n",
      "22124.38141465187\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/OCT_CAN.csv.gz\n",
      "22883.279091358185\n"
     ]
    }
   ],
   "source": [
    "#Main4\n",
    "#Doing the reindexing for all linkids (day-wise file)\n",
    "\n",
    "files = [os.path.join(CAN_AGG_PATH,filename) for filename in os.listdir(CAN_AGG_PATH)]\n",
    "files.sort()\n",
    "files = files[2:-1:2]\n",
    "\n",
    "for filename in files:\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    df = pd.read_csv(filename, compression='gzip')\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    \n",
    "    df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "    first_date = df['gps_timestamp'].iloc[0].date()\n",
    "    last_date = df['gps_timestamp'].iloc[-1].date()\n",
    "    dayslices = pd.date_range(first_date, last_date, freq='1D')\n",
    "    \n",
    "    for day in dayslices:\n",
    "        df_tmp = df[df['gps_timestamp'].dt.date==day]\n",
    "        timeslices = pd.date_range(day, day+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "        #mux = pd.MultiIndex.from_product([linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "        #df_tmp = df_tmp.set_index(['linkid', 'gps_timestamp']).reindex(mux).reset_index()\n",
    "        y = pd.DataFrame([], index=pd.MultiIndex.from_product([linkid_list, timeslices],names=['linkid', 'gps_timestamp']))\n",
    "        y = y.merge(df_tmp, on=['linkid', 'gps_timestamp'], how='left').reset_index().drop(['index'],axis=1)\n",
    "        y.to_csv(os.path.join(CAN_AGG_PATH,'CAN_Daywise_Reindexed',day.strftime('%Y%m'),day.strftime('%Y%m%d')+\n",
    "                                   '_CAN_Reindexed.csv.gz'), compression='gzip', index=False)\n",
    "\n",
    "    #print(df.linkid.nunique())\n",
    "    #print(len(df))\n",
    "    #print(df.speed_typea.isna().sum())\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data2/jiang/Toyota/JARTIC_data_202105/vics_regulation_202105_shutokou.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeph/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-01 00:00:00 2021-05-31 23:50:00\n",
      "6383.675914049149\n",
      "Processing /data2/jiang/Toyota/JARTIC_data_202110/vics_regulation_202110_C01_2.csv\n",
      "2021-10-01 00:00:00 2021-10-31 23:50:00\n",
      "5606.243230819702\n",
      "Processing /data2/jiang/Toyota/JARTIC_data_202111/vics_regulation_202111_C01.csv\n",
      "2021-11-01 00:00:00 2021-11-30 23:50:00\n",
      "6630.13353061676\n"
     ]
    }
   ],
   "source": [
    "#Main5\n",
    "#Creating Accident Tensor based on reindexed Capital linkids (monthly files)\n",
    "#/data2/jiang/Toyota/JARTIC_data_202105/vics_regulation_202105_shutokou.csv\n",
    "#/data2/jiang/Toyota/JARTIC_data_202110/vics_regulation_202110_C01_2.csv\n",
    "#/data2/jiang/Toyota/JARTIC_data_202111/vics_regulation_202111_C01.csv\n",
    "\n",
    "df_capital_link = pd.read_csv('/data2/jiang/Toyota/graph_data/capital_graph_link_info.csv')\n",
    "capital_linkid_list = df_capital_link['link_id'].unique()\n",
    "\n",
    "files = ['/data2/jiang/Toyota/JARTIC_data_202105/vics_regulation_202105_shutokou.csv',\n",
    "            '/data2/jiang/Toyota/JARTIC_data_202110/vics_regulation_202110_C01_2.csv',\n",
    "            '/data2/jiang/Toyota/JARTIC_data_202111/vics_regulation_202111_C01.csv']\n",
    "\n",
    "for i,filename in enumerate(files):\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    \n",
    "    if i==1:\n",
    "        df = pd.read_csv(filename, encoding='shift-jis')\n",
    "    else:\n",
    "        df = pd.read_csv(filename)\n",
    "    \n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    df['start_time'] = df['start_time'].dt.floor(SAMPLING_RATE)\n",
    "    df['end_time'] = df['end_time'].dt.floor(SAMPLING_RATE)\n",
    "    \n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[df['coord_start_upstream_nearestlink'].isin(capital_linkid_list)]\n",
    "    \n",
    "    #first_date = df['start_time'].iloc[0].date()\n",
    "    month = df['start_time'].iloc[0].month\n",
    "    year = df['start_time'].iloc[0].year\n",
    "    first_date = pd.to_datetime(str(year)+'-'+str(month)+'-01')\n",
    "    last_date = df['start_time'].max().date()\n",
    "    timeslices = pd.date_range(first_date, last_date+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "    print(timeslices[0], timeslices[-1])\n",
    "    \n",
    "    #mux = pd.MultiIndex.from_product([capital_linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "    y = pd.DataFrame([], index=pd.MultiIndex.from_product([capital_linkid_list, timeslices],\n",
    "                                                          names=['linkid', 'gps_timestamp'])).reset_index()\n",
    "    y['accident_flag'] = 0\n",
    "    #df['accident_flag'] = 1\n",
    "    for _,row in df.iterrows():\n",
    "        #y.loc[(row['coord_start_upstream_nearestlink'],row['start_time']):(row['coord_start_upstream_nearestlink'],\n",
    "        #                                                                   row['end_time']), 'accident_flag'] = 1\n",
    "        y['accident_flag'] = np.where((y['linkid']==row['coord_start_upstream_nearestlink']) & \n",
    "                (y['gps_timestamp']>=row['start_time']) & (y['gps_timestamp']<=row['end_time']), 1, y['accident_flag'])\n",
    "        \n",
    "    y.to_csv(CAN_AGG_PATH+'ACCIDENT_'+first_date.strftime('%Y-%m')+'_CAPITAL.csv.gz', compression='gzip', index=False)\n",
    "    \n",
    "    #print(df.linkid.nunique())\n",
    "    #print(len(df))\n",
    "    #print(df.speed_typea.isna().sum())\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2841"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(capital_linkid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/MAY_CAN_CAPITAL_Reindexed.csv.gz\n",
      "144.00749135017395\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/OCT_CAN_CAPITAL_Reindexed.csv.gz\n",
      "149.11396408081055\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/NOV_CAN_CAPITAL_Reindexed.csv.gz\n",
      "144.20460200309753\n"
     ]
    }
   ],
   "source": [
    "#Main6\n",
    "#Filling NaN values => (2) followed by (1)\n",
    "##(1): xxInterpolationxx Forward Fill based on last xxand nextxx available values (but it wouldn't be very accurate)\n",
    "##(2): Taking average of speed for that time period across all days (wherever data is available) for that particular linkid \n",
    "##     - combined with weekday and accident_flag information (there is possibility of having no value available)\n",
    "\n",
    "can_files = [CAN_AGG_PATH+'MAY_CAN_CAPITAL_Reindexed.csv.gz', CAN_AGG_PATH+'OCT_CAN_CAPITAL_Reindexed.csv.gz',\n",
    "             CAN_AGG_PATH+'NOV_CAN_CAPITAL_Reindexed.csv.gz']\n",
    "\n",
    "acc_files = [CAN_AGG_PATH+'ACCIDENT_2021-05_CAPITAL.csv.gz', CAN_AGG_PATH+'ACCIDENT_2021-10_CAPITAL.csv.gz',\n",
    "             CAN_AGG_PATH+'ACCIDENT_2021-11_CAPITAL.csv.gz']\n",
    "\n",
    "for can_file,acc_file in zip(can_files,acc_files):\n",
    "    tik = time.time()\n",
    "    print('Processing '+can_file)\n",
    "    df = pd.read_csv(can_file)\n",
    "    df['accident_flag'] = pd.read_csv(acc_file)['accident_flag']\n",
    "    \n",
    "    df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "    df['weekday'] = df['gps_timestamp'].dt.weekday\n",
    "    df['weekday'] = np.where(df['weekday']>=5, 0, 1)\n",
    "    df['interval'] = df.index%144\n",
    "    df['time'] = df['gps_timestamp'].dt.time\n",
    "    \n",
    "    #Method (2)\n",
    "    df_mean = df[df.accident_flag==0].groupby(by=['linkid','time','weekday']).speed_typea.mean().reset_index(name='speed_avg')\n",
    "    df = df.merge(df_mean, on=['linkid','time','weekday'], how='left')\n",
    "    df['speed_typea'] = np.where(df['accident_flag']==0, df['speed_typea'].fillna(df['speed_avg']), df['speed_typea'])\n",
    "    \n",
    "    #Method (1)\n",
    "    #if pd.isna(df['speed_typea'].iloc[0]):\n",
    "    #    df.loc[0, 'speed_typea'] = -2\n",
    "    df['speed_typea'] = np.where(df['speed_typea'].isna() & (df['linkid']!=df['linkid'].shift()), -2, df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(df['speed_typea'].isna() & (df['interval']==138), -2, df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(df['accident_flag']==0, df['speed_typea'].fillna(method='ffill'), df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(((df['speed_typea']==-2) | (df['speed_typea'].isna())) & (df['accident_flag']==1), \n",
    "                                         -1, df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(df['speed_typea']==-2, 100, df['speed_typea'])\n",
    "    \n",
    "    #Saving File\n",
    "    df[['linkid','gps_timestamp','speed_typea','accident_flag']].to_csv(can_file[:-7]+'_FilledNA.csv.gz',compression='gzip',\n",
    "                                                                       index=False)\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12682224"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409104"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.linkid.nunique()*144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data2/jiang/Toyota/JARTIC_data_202105/vics_accident_202105.csv\n",
      "2021-05-01 00:00:00 2021-05-31 23:50:00\n",
      "238.3499116897583\n",
      "Processing /data2/jiang/Toyota/JARTIC_data_202110/vics_accident_202110.csv\n",
      "2021-10-01 00:00:00 2021-10-31 23:50:00\n",
      "296.89824318885803\n",
      "Processing /data2/jiang/Toyota/JARTIC_data_202111/vics_accident_202111.csv\n",
      "2021-11-01 00:00:00 2021-11-30 23:50:00\n",
      "335.2698199748993\n"
     ]
    }
   ],
   "source": [
    "#Main7\n",
    "#Creating Real Accident (filtered from cause column) Tensor based on reindexed Capital linkids (monthly files)\n",
    "#/data2/jiang/Toyota/JARTIC_data_202105/vics_accident_202105.csv\n",
    "#/data2/jiang/Toyota/JARTIC_data_202110/vics_accident_202110.csv\n",
    "#/data2/jiang/Toyota/JARTIC_data_202111/vics_accident_202111.csv\n",
    "\n",
    "df_capital_link = pd.read_csv('/data2/jiang/Toyota/graph_data/capital_graph_link_info.csv')\n",
    "capital_linkid_list = df_capital_link['link_id'].unique()\n",
    "\n",
    "files = ['/data2/jiang/Toyota/JARTIC_data_202105/vics_accident_202105.csv', \n",
    "         '/data2/jiang/Toyota/JARTIC_data_202110/vics_accident_202110.csv',\n",
    "         '/data2/jiang/Toyota/JARTIC_data_202111/vics_accident_202111.csv']\n",
    "\n",
    "for filename in files:\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    df['start_time'] = df['start_time'].dt.floor(SAMPLING_RATE)\n",
    "    df['end_time'] = df['end_time'].dt.floor(SAMPLING_RATE)\n",
    "    \n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[df['coord_start_upstream_nearestlink'].isin(capital_linkid_list)]\n",
    "    \n",
    "    #first_date = df['start_time'].iloc[0].date()\n",
    "    month = df['start_time'].iloc[0].month\n",
    "    year = df['start_time'].iloc[0].year\n",
    "    first_date = pd.to_datetime(str(year)+'-'+str(month)+'-01')\n",
    "    last_date = df['start_time'].max().date()\n",
    "    timeslices = pd.date_range(first_date, last_date+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "    print(timeslices[0], timeslices[-1])\n",
    "    \n",
    "    #mux = pd.MultiIndex.from_product([capital_linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "    y = pd.DataFrame([], index=pd.MultiIndex.from_product([capital_linkid_list, timeslices],\n",
    "                                                          names=['linkid', 'gps_timestamp'])).reset_index()\n",
    "    y['real_accident_flag'] = 0\n",
    "    #df['accident_flag'] = 1\n",
    "    for _,row in df.iterrows():\n",
    "        #y.loc[(row['coord_start_upstream_nearestlink'],row['start_time']):(row['coord_start_upstream_nearestlink'],\n",
    "        #                                                                   row['end_time']), 'accident_flag'] = 1\n",
    "        y['real_accident_flag'] = np.where((y['linkid']==row['coord_start_upstream_nearestlink']) & \n",
    "                (y['gps_timestamp']>=row['start_time']) & (y['gps_timestamp']<=row['end_time']), 1, y['real_accident_flag'])\n",
    "        \n",
    "    y.to_csv(CAN_AGG_PATH+'REAL_ACCIDENT_'+first_date.strftime('%Y-%m')+'_CAPITAL.csv.gz', compression='gzip', index=False)\n",
    "    \n",
    "    #print(df.linkid.nunique())\n",
    "    #print(len(df))\n",
    "    #print(df.speed_typea.isna().sum())\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/MAY_CAN_CAPITAL_Reindexed_FilledNA.csv.gz\n",
      "121.92135000228882\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/OCT_CAN_CAPITAL_Reindexed_FilledNA.csv.gz\n",
      "126.28425812721252\n",
      "Processing /data2/jiang/workToyota/data/CAN_Aggregated/NOV_CAN_CAPITAL_Reindexed_FilledNA.csv.gz\n",
      "122.5358817577362\n"
     ]
    }
   ],
   "source": [
    "#Main8\n",
    "#Merging the FilledNA files with real_accident_flag data\n",
    "#columns -> linkid, gps_timestamp, speed_typea, accident_flag, realaccident_flag\n",
    "#or you can rename them into:\n",
    "#linkid, gps_timestamp, speed_typea, incident_flag, accident_flag\n",
    "#â€»here the incident_flag corresponds to the original accident_flag, accident_flag is the newly added realaccident flag.\n",
    "#if realaccident_flag=1, accident_flag must =1, but not vise versa. \n",
    "#Because the realaccident data is a pure subset of accident data (incident data).\n",
    "\n",
    "can_files = [CAN_AGG_PATH+'MAY_CAN_CAPITAL_Reindexed_FilledNA.csv.gz', CAN_AGG_PATH+'OCT_CAN_CAPITAL_Reindexed_FilledNA.csv.gz',\n",
    "             CAN_AGG_PATH+'NOV_CAN_CAPITAL_Reindexed_FilledNA.csv.gz']\n",
    "\n",
    "acc_files = [CAN_AGG_PATH+'REAL_ACCIDENT_2021-05_CAPITAL.csv.gz', CAN_AGG_PATH+'REAL_ACCIDENT_2021-10_CAPITAL.csv.gz',\n",
    "             CAN_AGG_PATH+'REAL_ACCIDENT_2021-11_CAPITAL.csv.gz']\n",
    "\n",
    "for can_file,acc_file in zip(can_files,acc_files):\n",
    "    tik = time.time()\n",
    "    print('Processing '+can_file)\n",
    "    df = pd.read_csv(can_file)\n",
    "    df['real_accident_flag'] = pd.read_csv(acc_file)['real_accident_flag']\n",
    "    \n",
    "    #Saving File\n",
    "    df.to_csv(can_file[:-7]+'_IncidentAccident.csv.gz',compression='gzip', index=False)\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
