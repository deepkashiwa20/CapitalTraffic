{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "MAY_CAN_PATH = '/data2/jiang/Toyota/CAN_day_data_1122ver/'\n",
    "OCT_CAN_PATH = '/data2/jiang/Toyota/CAN_day_data_202110/'\n",
    "NOV_CAN_PATH = '/data2/jiang/Toyota/CAN_day_data_202111/'\n",
    "CAN_AGG_PATH = '/data2/jiang/workToyota/data/CAN_Aggregated/'\n",
    "CAPITAL_LINK_FILE = '/data2/jiang/Toyota/graph_data/capital_graph_link_info.csv'\n",
    "SAMPLING_RATE = '10min'      #Defining time-interval for aggregation\n",
    "LAT_MIN = 35.36\n",
    "LAT_MAX = 35.90\n",
    "LON_MIN = 139.537\n",
    "LON_MAX = 139.947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract linkids in give rectangular coordinates\n",
    "def clip_CAN(df):\n",
    "    #df = pd.read_csv(os.path.join(NEW_LINK_PATH, 'link_connect_all.csv'))\n",
    "    #df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['start_lon'], df['start_lat'])).set_crs(epsg=4326)\n",
    "    dft = df[(df['mmlatitude']>=LAT_MIN) & (df['mmlatitude']<=LAT_MAX) & (df['mmlongitude']>=LON_MIN) & \n",
    "            (df['mmlongitude']<=LON_MAX)]\n",
    "    #df = gpd.clip(df, mask)\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function1- (Aggregating the avg speed for each linkid and saving in monthly files (without reindexing))\n",
    "def Aggregate_AvgSpeed_CAN(PATH_VAR, month_var):\n",
    "    \n",
    "    #mask = Polygon([(LON_MIN,LAT_MAX), (LON_MAX,LAT_MAX), (LON_MAX,LAT_MIN), (LON_MIN,LAT_MIN)])\n",
    "    \n",
    "    files = [os.path.join(PATH_VAR+filename) for filename in os.listdir(PATH_VAR)]\n",
    "    files.sort()\n",
    "    \n",
    "    tik = time.time()\n",
    "    df_month = []\n",
    "    for filename in files:\n",
    "        #Reading the file, dropping NA values, and clipping it\n",
    "        df = pd.read_csv(filename, compression='gzip')\n",
    "        df = df.dropna(subset=['speed_typea'])\n",
    "        df = clip_CAN(df)\n",
    "        \n",
    "        #Sampling and getting avg speed after aggregation\n",
    "        df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "        df['gps_timestamp'] = df['gps_timestamp'].dt.floor(SAMPLING_RATE)    \n",
    "        df = df.groupby(['linkid','gps_timestamp'], as_index=False)['speed_typea'].mean()[['linkid',\"gps_timestamp\",\n",
    "                                                                                         \"speed_typea\"]]\n",
    "\n",
    "        #Appending the one day result\n",
    "        df_month.append(df)\n",
    "    \n",
    "    del df\n",
    "    df_month = pd.concat(df_month)\n",
    "    df_month.to_csv(CAN_AGG_PATH+month_var+'_CAN.csv.gz', compression='gzip', index=False)\n",
    "    \n",
    "    tok = time.time()\n",
    "    #print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function2\n",
    "#Getting combined list of all unique linkids from all monthly files\n",
    "def get_linkid_list(can_files):\n",
    "    files = [os.path.join(CAN_AGG_PATH,filename) for filename in can_files]\n",
    "    \n",
    "    linkid_list = []\n",
    "\n",
    "    for filename in files:\n",
    "        tik = time.time()\n",
    "        print('Processing '+filename)\n",
    "        df = pd.read_csv(filename, compression='gzip')\n",
    "        linkid_list = linkid_list + df.linkid.unique().tolist()\n",
    "        tok = time.time()\n",
    "        #print(tok-tik)\n",
    "    \n",
    "    linkid_list = np.unique(np.array(linkid_list))\n",
    "    return linkid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function 3\n",
    "#Doing the reindexing for all linkids (saving in day-wise file)\n",
    "def reindex_can_day(filename, linkid_list):\n",
    "    #files = [os.path.join(CAN_AGG_PATH,filename) for filename in can_files]\n",
    "    filename = os.path.join(CAN_AGG_PATH, filename)\n",
    "    \n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    df = pd.read_csv(filename, compression='gzip')\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    \n",
    "    df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "    first_date = df['gps_timestamp'].iloc[0].date()\n",
    "    last_date = df['gps_timestamp'].iloc[-1].date()\n",
    "    dayslices = pd.date_range(first_date, last_date, freq='1D')\n",
    "    \n",
    "    for day in dayslices:\n",
    "        df_tmp = df[df['gps_timestamp'].dt.date==day]\n",
    "        timeslices = pd.date_range(day, day+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "        #mux = pd.MultiIndex.from_product([linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "        #df_tmp = df_tmp.set_index(['linkid', 'gps_timestamp']).reindex(mux).reset_index()\n",
    "        y = pd.DataFrame([], index=pd.MultiIndex.from_product([linkid_list, timeslices],names=['linkid', 'gps_timestamp']))\n",
    "        y = y.merge(df_tmp, on=['linkid', 'gps_timestamp'], how='left').reset_index().drop(['index'],axis=1)\n",
    "        y.to_csv(os.path.join(CAN_AGG_PATH,'CAN_Daywise_Reindexed',day.strftime('%Y%m'),day.strftime('%Y%m%d')+\n",
    "                                   '_CAN_Reindexed.csv.gz'), compression='gzip', index=False)\n",
    "\n",
    "    tok = time.time()\n",
    "    #print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function4\n",
    "#Extracting capital linkids only and doing the reindexing (saving in monthly files)\n",
    "def reindex_can_capital(filename, capital_linkid_list):\n",
    "    \n",
    "    filename = os.path.join(CAN_AGG_PATH, filename)\n",
    "\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    \n",
    "    df = pd.read_csv(filename, compression='gzip')\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[df['linkid'].isin(capital_linkid_list)]\n",
    "    #print(len(df))\n",
    "    \n",
    "    df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "    first_date = df['gps_timestamp'].iloc[0].date()\n",
    "    last_date = df['gps_timestamp'].iloc[-1].date()\n",
    "    timeslices = pd.date_range(first_date, last_date+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "    \n",
    "    mux = pd.MultiIndex.from_product([capital_linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "    df = df.set_index(['linkid', 'gps_timestamp']).reindex(mux).reset_index()\n",
    "    df.to_csv(filename[:-7]+'_CAPITAL_Reindexed.csv.gz', compression='gzip', index=False)\n",
    "    \n",
    "    #print(df.linkid.nunique())\n",
    "    #print(len(df))\n",
    "    #print(df.speed_typea.isna().sum())\n",
    "    tok = time.time()\n",
    "    #print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function5\n",
    "#Creating Accident Tensor based on reindexed Capital linkids (monthly files)\n",
    "def create_accident_tensor_capital(filename, capital_linkid_list):\n",
    "\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "    except:\n",
    "        df = pd.read_csv(filename, encoding='shift-jis')\n",
    "    \n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    df['start_time'] = df['start_time'].dt.floor(SAMPLING_RATE)\n",
    "    df['end_time'] = df['end_time'].dt.floor(SAMPLING_RATE)\n",
    "    \n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[df['coord_start_upstream_nearestlink'].isin(capital_linkid_list)]\n",
    "    \n",
    "    #first_date = df['start_time'].iloc[0].date()\n",
    "    month = df['start_time'].iloc[0].month\n",
    "    year = df['start_time'].iloc[0].year\n",
    "    first_date = pd.to_datetime(str(year)+'-'+str(month)+'-01')\n",
    "    last_date = df['start_time'].max().date()\n",
    "    timeslices = pd.date_range(first_date, last_date+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "    #print(timeslices[0], timeslices[-1])\n",
    "    \n",
    "    #mux = pd.MultiIndex.from_product([capital_linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "    y = pd.DataFrame([], index=pd.MultiIndex.from_product([capital_linkid_list, timeslices],\n",
    "                                                          names=['linkid', 'gps_timestamp'])).reset_index()\n",
    "    y['accident_flag'] = 0\n",
    "    for _,row in df.iterrows():\n",
    "        #y.loc[(row['coord_start_upstream_nearestlink'],row['start_time']):(row['coord_start_upstream_nearestlink'],\n",
    "        #                                                                   row['end_time']), 'accident_flag'] = 1\n",
    "        y['accident_flag'] = np.where((y['linkid']==row['coord_start_upstream_nearestlink']) & \n",
    "                (y['gps_timestamp']>=row['start_time']) & (y['gps_timestamp']<=row['end_time']), 1, y['accident_flag'])\n",
    "        \n",
    "    y.to_csv(CAN_AGG_PATH+'ACCIDENT_'+first_date.strftime('%Y-%m')+'_CAPITAL.csv.gz', compression='gzip', index=False)\n",
    "    \n",
    "    #print(df.linkid.nunique())\n",
    "    #print(len(df))\n",
    "    #print(df.speed_typea.isna().sum())\n",
    "    tok = time.time()\n",
    "    #print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function6\n",
    "#Filling NaN values => (2) followed by (1)\n",
    "##(1): xxInterpolationxx Forward Fill followed by bfill (daily basis) based on last xxand nextxx available values (but it \n",
    "##     wouldn't be very accurate)\n",
    "##(2): Taking average of speed for that time period across all days (wherever data is available) for that particular linkid \n",
    "##     - combined with weekday and accident_flag information (there is possibility of having no value available)\n",
    "def fill_nan_can_capital(can_file, acc_file):\n",
    "\n",
    "    tik = time.time()\n",
    "    print('Processing '+can_file)\n",
    "    df = pd.read_csv(can_file)\n",
    "    df['accident_flag'] = pd.read_csv(acc_file)['accident_flag']\n",
    "    \n",
    "    df['gps_timestamp'] = pd.to_datetime(df['gps_timestamp'])\n",
    "    df['weekday'] = df['gps_timestamp'].dt.weekday\n",
    "    df['weekday'] = np.where(df['weekday']>=5, 0, 1)\n",
    "    df['interval'] = df.index%144\n",
    "    df['time'] = df['gps_timestamp'].dt.time\n",
    "    \n",
    "    #Method (2)\n",
    "    df_mean = df[df.accident_flag==0].groupby(by=['linkid','time','weekday']).speed_typea.mean().reset_index(name='speed_avg')\n",
    "    df = df.merge(df_mean, on=['linkid','time','weekday'], how='left')\n",
    "    df['speed_typea'] = np.where(df['accident_flag']==0, df['speed_typea'].fillna(df['speed_avg']), df['speed_typea'])\n",
    "    \n",
    "    #Method (1)\n",
    "    df['speed_typea'] = np.where(df['speed_typea'].isna() & (df['interval']==0), -2, df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(df['accident_flag']==0, df['speed_typea'].fillna(method='ffill'), df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(df['speed_typea']==-2, np.nan, df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(df['accident_flag']==0, df['speed_typea'].fillna(method='bfill'), df['speed_typea'])\n",
    "    df['speed_typea'] = np.where(df['speed_typea'].isna() & (df['accident_flag']==1), -1, df['speed_typea'])\n",
    "    \n",
    "    #Saving File\n",
    "    df[['linkid','gps_timestamp','speed_typea','accident_flag']].to_csv(can_file[:-7]+'_FilledNA.csv.gz',compression='gzip',\n",
    "                                                                       index=False)\n",
    "    tok = time.time()\n",
    "    #print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function7\n",
    "#Creating Real Accident (filtered from cause column) Tensor based on reindexed Capital linkids (monthly files)\n",
    "def create_real_accident_tensor_capital(filename, capital_linkid_list):\n",
    "    tik = time.time()\n",
    "    print('Processing '+filename)\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    df['start_time'] = df['start_time'].dt.floor(SAMPLING_RATE)\n",
    "    df['end_time'] = df['end_time'].dt.floor(SAMPLING_RATE)\n",
    "    \n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[df['coord_start_upstream_nearestlink'].isin(capital_linkid_list)]\n",
    "    \n",
    "    #first_date = df['start_time'].iloc[0].date()\n",
    "    month = df['start_time'].iloc[0].month\n",
    "    year = df['start_time'].iloc[0].year\n",
    "    first_date = pd.to_datetime(str(year)+'-'+str(month)+'-01')\n",
    "    last_date = df['start_time'].max().date()\n",
    "    timeslices = pd.date_range(first_date, last_date+dt.timedelta(days=1), freq=SAMPLING_RATE)[:-1]\n",
    "    print(timeslices[0], timeslices[-1])\n",
    "    \n",
    "    #mux = pd.MultiIndex.from_product([capital_linkid_list, timeslices],names=['linkid', 'gps_timestamp'])\n",
    "    y = pd.DataFrame([], index=pd.MultiIndex.from_product([capital_linkid_list, timeslices],\n",
    "                                                          names=['linkid', 'gps_timestamp'])).reset_index()\n",
    "    y['real_accident_flag'] = 0\n",
    "    #df['accident_flag'] = 1\n",
    "    for _,row in df.iterrows():\n",
    "        #y.loc[(row['coord_start_upstream_nearestlink'],row['start_time']):(row['coord_start_upstream_nearestlink'],\n",
    "        #                                                                   row['end_time']), 'accident_flag'] = 1\n",
    "        y['real_accident_flag'] = np.where((y['linkid']==row['coord_start_upstream_nearestlink']) & \n",
    "                (y['gps_timestamp']>=row['start_time']) & (y['gps_timestamp']<=row['end_time']), 1, y['real_accident_flag'])\n",
    "        \n",
    "    y.to_csv(CAN_AGG_PATH+'REAL_ACCIDENT_'+first_date.strftime('%Y-%m')+'_CAPITAL.csv.gz', compression='gzip', index=False)\n",
    "    \n",
    "    tok = time.time()\n",
    "    #print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function8\n",
    "#Merging the FilledNA files with real_accident_flag data\n",
    "#columns -> linkid, gps_timestamp, speed_typea, accident_flag, realaccident_flag\n",
    "#â€»here the accident_flag corresponds to the original incident data, while real_accident_flag is the newly added \n",
    "#  real_accident flag.\n",
    "#if realaccident_flag=1, accident_flag must =1, but not vise versa. \n",
    "#Because the realaccident data is a pure subset of accident data (incident data).\n",
    "def merge_fillednan_realaccident(can_file, acc_file):\n",
    "    tik = time.time()\n",
    "    print('Processing '+can_file)\n",
    "    df = pd.read_csv(can_file)\n",
    "    df['real_accident_flag'] = pd.read_csv(acc_file)['real_accident_flag']\n",
    "    \n",
    "    #Saving File\n",
    "    df.to_csv(can_file[:-7]+'_IncidentAccident.csv.gz',compression='gzip', index=False)\n",
    "    tok = time.time()\n",
    "    #print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Aggregate_AvgSpeed_CAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-09e20d3fd345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Aggregating the average speed for each linkid in CAN files and saving them without reindexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mAggregate_AvgSpeed_CAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAY_CAN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mAggregate_AvgSpeed_CAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOCT_CAN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OCT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mAggregate_AvgSpeed_CAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNOV_CAN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NOV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Aggregate_AvgSpeed_CAN' is not defined"
     ]
    }
   ],
   "source": [
    "#Main Function\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    #1-Aggregating the average speed for each linkid in CAN files and saving them in monthly files without reindexing\n",
    "    Aggregate_AvgSpeed_CAN(MAY_CAN_PATH, 'MAY')\n",
    "    Aggregate_AvgSpeed_CAN(OCT_CAN_PATH, 'OCT')\n",
    "    Aggregate_AvgSpeed_CAN(NOV_CAN_PATH, 'NOV')\n",
    "    \n",
    "    #2-Get combined unique LinkID list\n",
    "    can_files = ['MAY_CAN.csv.gz', 'OCT_CAN.csv.gz', 'NOV_CAN.csv.gz']\n",
    "    linkid_list = get_linkid_list(can_files)\n",
    "    \n",
    "    #3-Reindex the monthly aggregated CAN files with above linkid list for all possible time-slots (day-wise)\n",
    "    reindex_can_day(can_files[0], linkid_list)\n",
    "    reindex_can_day(can_files[1], linkid_list)\n",
    "    reindex_can_day(can_files[2], linkid_list)\n",
    "    \n",
    "    #4-Reindexing the monthly aggregated CAN for caital links only (month-wise) \n",
    "    df_capital_link = pd.read_csv(CAPITAL_LINK_FILE)\n",
    "    capital_linkid_list = df_capital_link['link_id'].unique()\n",
    "    reindex_can_capital(can_files[0], capital_linkid_list)\n",
    "    reindex_can_capital(can_files[1], capital_linkid_list)\n",
    "    reindex_can_capital(can_files[2], capital_linkid_list)\n",
    "\n",
    "    #5-Creating accident tensors for capital linkids (monthly files)\n",
    "    acc_files = ['/data2/jiang/Toyota/JARTIC_data_202105/vics_regulation_202105_shutokou.csv',\n",
    "            '/data2/jiang/Toyota/JARTIC_data_202110/vics_regulation_202110_C01_2.csv',\n",
    "            '/data2/jiang/Toyota/JARTIC_data_202111/vics_regulation_202111_C01.csv']\n",
    "    create_accident_tensor_capital(acc_files[0], capital_linkid_list)\n",
    "    create_accident_tensor_capital(acc_files[1], capital_linkid_list)\n",
    "    create_accident_tensor_capital(acc_files[2], capital_linkid_list)\n",
    "    \n",
    "    #6-Filling NaN values in reindexed can capital files\n",
    "    can_capital_files = [CAN_AGG_PATH+'MAY_CAN_CAPITAL_Reindexed.csv.gz', CAN_AGG_PATH+'OCT_CAN_CAPITAL_Reindexed.csv.gz',\n",
    "             CAN_AGG_PATH+'NOV_CAN_CAPITAL_Reindexed.csv.gz']\n",
    "    acc_tensor_files = [CAN_AGG_PATH+'ACCIDENT_2021-05_CAPITAL.csv.gz', CAN_AGG_PATH+'ACCIDENT_2021-10_CAPITAL.csv.gz',\n",
    "             CAN_AGG_PATH+'ACCIDENT_2021-11_CAPITAL.csv.gz']\n",
    "    fill_nan_can_capital(can_capital_files[0], acc_tensor_files[0])\n",
    "    fill_nan_can_capital(can_capital_files[1], acc_tensor_files[1])\n",
    "    fill_nan_can_capital(can_capital_files[2], acc_tensor_files[2])\n",
    "    \n",
    "    #7-Create Real Accident Tensor for capital linkids (monthly files)\n",
    "    real_acc_files = ['/data2/jiang/Toyota/JARTIC_data_202105/vics_accident_202105.csv', \n",
    "                      '/data2/jiang/Toyota/JARTIC_data_202110/vics_accident_202110.csv',\n",
    "                      '/data2/jiang/Toyota/JARTIC_data_202111/vics_accident_202111.csv']\n",
    "    create_real_accident_tensor_capital(real_acc_files[0], capital_linkid_list)\n",
    "    create_real_accident_tensor_capital(real_acc_files[1], capital_linkid_list)\n",
    "    create_real_accident_tensor_capital(real_acc_files[2], capital_linkid_list)\n",
    "    \n",
    "    #8-Merging the filledNA files with real_accident_flag data\n",
    "    filledna_can_files = [CAN_AGG_PATH+'MAY_CAN_CAPITAL_Reindexed_FilledNA.csv.gz', \n",
    "                          CAN_AGG_PATH+'OCT_CAN_CAPITAL_Reindexed_FilledNA.csv.gz',\n",
    "                          CAN_AGG_PATH+'NOV_CAN_CAPITAL_Reindexed_FilledNA.csv.gz']\n",
    "    real_acc_tensor_files = [CAN_AGG_PATH+'REAL_ACCIDENT_2021-05_CAPITAL.csv.gz', \n",
    "                             CAN_AGG_PATH+'REAL_ACCIDENT_2021-10_CAPITAL.csv.gz',\n",
    "                             CAN_AGG_PATH+'REAL_ACCIDENT_2021-11_CAPITAL.csv.gz']\n",
    "    merge_fillednan_realaccident(filledna_can_files[0], real_acc_tensor_files[0])\n",
    "    merge_fillednan_realaccident(filledna_can_files[1], real_acc_tensor_files[1])\n",
    "    merge_fillednan_realaccident(filledna_can_files[2], real_acc_tensor_files[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
